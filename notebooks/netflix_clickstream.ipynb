{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Netflix Clickstream Dataset into Pandas\n",
    "\n",
    "The dataset is stored split up into many Parquet files in S3 (over 40,000 of them).\n",
    "Parquet is a file format often used for storing large amounts of data efficiently.\n",
    "\n",
    "Nonetheless, it will not be practical (or possible) to load all of the data onto your local machine and work with it in Pandas, as the following will demonstrate. \n",
    "To start gleaning insights from large datasets, data analysts usually choose a sample of the data to work with. This avoids having to downloading large amounts of files that may not fit onto a laptop's disk or into memory.\n",
    "\n",
    "This notebook illustrates:\n",
    "\n",
    "1. How to load a single Parquet file from S3 into Pandas using the `pd.read_parquet` function.\n",
    "2. How to define a sample of files from S3 using `s3fs` and `pd.read_parquet`.\n",
    "3. How to save the contents of DataFrames to your laptop to avoid having to load large datasets from the Cloud repeatedly.\n",
    "4. How to restrict the set of columns to load into Pandas from S3.\n",
    "\n",
    "The following assumes that you have `s3fs` and `pyarrow` installed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset on S3\n",
    "\n",
    "Files in S3 have addresses that look like `s3://s3-bucket-name/some/path/to/the/file`. The Netflix clickstream dataset is stored in under `s3://data-eng-datasets-404544469985/vod_clickstream/full/`.\n",
    "\n",
    "We can list the files in the dataset by running the following command in a terminal:\n",
    "\n",
    "```\n",
    "aws s3 ls s3://data-eng-datasets-404544469985/vod_clickstream/full/ --recursive\n",
    "```\n",
    "\n",
    "This will list all of the files that make of up the dataset. The output will be very long.\n",
    "Run the next cell to see an example (yes you can use `!` to run commands from Jupyter notebooks!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://data-eng-datasets-404544469985/vod_clickstream/full/ --recursive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But how do we get these files into Pandas? There are a couple of bits to it:\n",
    "\n",
    "- How to read Parquet files using Pandas\n",
    "- How to read data from S3\n",
    "- How to get a subset of the files into Pandas to avoid running out of space and memory on your laptop\n",
    "\n",
    "Read on to find out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a single Parquet file from S3 into Pandas\n",
    "\n",
    "We can use [`pd.read_parquet`](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) to load a Parquet file into a Pandas DataFrame.\n",
    "With `s3fs` installed, it's enough to supply the S3 path to the file (and we can get paths to files using `aws s3 ls` as illustrated above). The file will be downloaded from S3 to your laptop, then loaded into a DataFrame.\n",
    "\n",
    "Here's an example of using `pd.read_parquet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "netflix_sample_df = pd.read_parquet(\n",
    "    f\"s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0000_part_00.parquet\",\n",
    ")\n",
    "netflix_sample_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've got some data. The meaning of the columns will probably not be clear to you yet, but that's expected. \n",
    "You'll have to chance to dive into that later on.\n",
    "\n",
    "For now, let's work out how we can load multiple files into Pandas at once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a subset of files to load into Pandas from S3\n",
    "\n",
    "One approach to loading multiple files into a DataFrame would be to first figure out which files we want, then load them all into separate DataFrames as shown above, and finally to join all these DataFrames into one large one. But that's cumbersome!\n",
    "\n",
    "Luckily, there are easier ways.\n",
    "One of them is to use a [glob pattern](https://en.wikipedia.org/wiki/Glob_(programming)). \n",
    "Glob patterns allow us to specify sets of filenames with wildcard characters.\n",
    "\n",
    "You can try this on your machine now. Navigate to a location where you have some Python files stored on your laptop, then list the files in that directory\n",
    "\n",
    "```\n",
    "cd path/to/some/python/project\n",
    "ls\n",
    "```\n",
    "\n",
    "This will show all the files you have in your project.\n",
    "But what if you only wanted to list the Python files? Run the following:\n",
    "\n",
    "```\n",
    "ls *.py\n",
    "```\n",
    "\n",
    "This will list only the files that end in `.py`.\n",
    "Here, `*` is a wildcard standing for \"any string of characters except /\" and `*.py` is a glob pattern.\n",
    "\n",
    "We're going to use glob patterns and the `s3fs` library to select a sample of the full dataset from S3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using glob patterns to select multiple S3 files at once\n",
    "\n",
    "The files of our dataset have S3 filepaths of the form:\n",
    "\n",
    "```\n",
    "s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0000_part_00.parquet\n",
    "```\n",
    "\n",
    "Note that the filepaths includes the date as `dt=2016-01-01`, meaning that this particular file contains user activity from the 1st January 2016. Let's say we wanted to focus our analysis on the January 1st 2016 only.\n",
    "\n",
    "To make a glob pattern that covers all of the Parquet files for January 1st, we'll have to replace the filename with `*`:\n",
    "\n",
    "```\n",
    "s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/*.parquet\n",
    "```\n",
    "\n",
    "And because we know all the files end in `.parquet`, we can simplify the above to omit that part and end up with this glob pattern:\n",
    "\n",
    "```\n",
    "s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/*\n",
    "```\n",
    "\n",
    "This will match all files that contain user activity from January 1st 2016.\n",
    "Now, let's use `s3fs` to list out all of the matching files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files you are interested in.\n",
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/*')\n",
    "len(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above cell should print the number 32, meaning that `s3fs` has found 32 files. We can inspect the filenames to see if they look correct.\n",
    "\n",
    "> Note: this might take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0000_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0001_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0002_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0003_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0004_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0005_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0006_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0007_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0008_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0009_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0010_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0011_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0012_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0013_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0014_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0015_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0016_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0017_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0018_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0019_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0020_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0021_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0022_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0023_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0024_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0025_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0026_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0027_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0028_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0029_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0030_part_00.parquet',\n",
       " 'data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0031_part_00.parquet']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of file paths like this one:\n",
    "\n",
    "```\n",
    "data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0000_part_00.parquet\n",
    "```\n",
    "\n",
    "You might notice that the `s3://` prefix is missing. But that's okay. It just means that we have to remember to supply it when we pass these filenames to pandas.\n",
    "\n",
    "Let's do that next. We'll read each of these files into a separate DataFrame using a for loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read them into pandas + concat the dfs\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_parquet(f\"s3://{file}\")\n",
    "    dfs.append(df)\n",
    "len(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have data across 32 different DataFrames. We can concatenate them all into a single DataFrame using [pd.concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_2016jan1_df = pd.concat(dfs)\n",
    "netflix_2016jan1_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using s3fs and glob patterns, we've loaded a whole day of data into Pandas without having to manually write out the name of each file we wanted to load."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading more than one day of data\n",
    "\n",
    "On my machine, it took 6 seconds for all of these files to be loaded, which is longer than you're probably used to waiting for Pandas to load a dataset. But a day of data isn't even all that much. \n",
    "\n",
    "As an experiment, the next cell loads the whole month of January using the following glob pattern `s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-*/*`.\n",
    "\n",
    "The code below is likely to take more than 5 minutes to run because it will take a while to download these files from S3.\n",
    "Longer waiting times is often something we have to contend with when working with large datasets locally.\n",
    "For this reason, let's also add some progress indicators so we have an idea of what's going on while we wait for it to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "january_2016_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-*/*')\n",
    "print(f\"Found {len(january_2016_files)} files.\")\n",
    "\n",
    "dfs = []\n",
    "for i, file in enumerate(january_2016_files):\n",
    "    print(f\"Loading file {i + 1}/{len(january_2016_files)} into a DataFrame ...\")\n",
    "    df = pd.read_parquet(f\"s3://{file}\")\n",
    "    dfs.append(df)\n",
    "len(dfs)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's concatenate the resulting DataFrames again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january_2016 = pd.concat(dfs)\n",
    "df_january_2016"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe has more than 4.5 million rows. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your work\n",
    "\n",
    "To avoid having to download the data again (you might accidentally close your notebook at the end of a day or overwrite your variables), it's a good idea to save the contents of the DataFrame to your laptop locally so you can pick up from where you left off:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january_2016.to_parquet(\"netflix_jan_2016.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code saves the dataframe as a Parquet file named `netflix_jan_2016.parquet` in the current directory. This is especiallly useful if you've ended up making some transformations to the data in the DataFrame and you don't want to have to start from scratch. \n",
    "\n",
    "Next time you want to load the data, you can just use `pd.from_parquet(\"netflix_jan_2016.parquet\")` to retrieve it without having to download it again from S3.\n",
    "\n",
    "> Note: You don't have to save the data as a Parquet file. Pandas has functions for saving to [CSV](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html), Parquet and many other formats. But for large DataFrames, Parquet is probably the best choice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More glob patterns\n",
    "\n",
    "We've seen how to select all files for a specific date and for a whole month:\n",
    "\n",
    "```\n",
    "jan1_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/*')\n",
    "all_jan_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-*/*')\n",
    "```\n",
    "\n",
    "To express more complex conditions, you can use `[]` to express a range of acceptable characters. For example, this glob pattern matches the first 3 months of 2016:\n",
    "\n",
    "```\n",
    "2016q1_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-0[1-3]-*/*')\n",
    "```\n",
    "\n",
    "`[...]` can also be used to list a set of characters to match. The following pattern matches January and May 2016: \n",
    "\n",
    "```\n",
    "2016_jan_may_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-0[15]-*/*')\n",
    "```\n",
    "\n",
    "Not everything will be easily expressible using a single glob pattern. \n",
    "You can always combine file lists generated by separate globs by concatenating them:\n",
    "\n",
    "```\n",
    "2016_jan_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-*/*')\n",
    "2017_dec_files = s3.glob('s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2017-12-*/*')\n",
    "\n",
    "all_files = 2016_jan_files + 2017_dec_files\n",
    "```\n",
    "\n",
    "Note that all of these may take quite a significant time to load. It's best to get started with a small sample that does download in an acceptable time and see what you can glean from it.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A convenient function for loading samples\n",
    "\n",
    "For your convenience, below is a function that encapsulates the code that loads files using a glob pattern into a DataFrame and prints out some progress indicators. Feel free to use it in your own analysis and adapt it for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "\n",
    "def load_parquet_files_from_s3(glob_pattern):\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "    files = s3.glob(glob_pattern)\n",
    "    print(f\"Found {len(files)} files.\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"Loading file {i + 1}/{len(files)} into a DataFrame ...\")\n",
    "        df = pd.read_parquet(f\"s3://{file}\")\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "df_jan1_2016 = load_parquet_files_from_s3(\"s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan1_2016.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricting data size by filtering out columns we don't need\n",
    "\n",
    "Another challenge of working with large datasets is that large Pandas DataFrames can become unwiedly. Holding on to a large Pandas DataFrame can make your computer slower. For example, running Pandas operations like `.groupby` and other common functions can take a while.\n",
    "\n",
    "This is because large DataFrames take up a lot of space in your computer's working memory, which leaves less space for storing temporary results of any other operations you're asking the computer to do. Those temporary results then have to be stored in slower forms of storage like SSD or hard disk.\n",
    "\n",
    "Running `.info()` on a DataFrame can give you a sense of this. It prints out a bunch of information including how much memory the DataFrame is taking up in the last line of the output:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january_2016.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the bottom of the output, you'll see a line like `memory usage: 2.0+ GB`. \n",
    "The DataFrame for January occupies more than 2 GB in memory. One way to reduce memory usage is to avoid loading columns we don't need.\n",
    "Looking at the DataFrame we already loaded, we can see that the dataset has a lot of columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_january_2016.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january_2016.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need all of these? Probably not.\n",
    "You might want to refine this selection some more when you start exploring the data by yourself, but for now, let's restrict to the ones that look the most interesting:\n",
    "\n",
    "- The time and date of the user action (`datetime`)\n",
    "- The URL the user visited (`event_url`)\n",
    "- The user ID (`panelist_id`)\n",
    "- The user's country (`server_request_country_code`)\n",
    "\n",
    "Let's create a dataframe that only has those 4 columns and compare its memory usage to that of the original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january_2016_smaller = df_january_2016[[\"datetime\", \"event_url\", \"panelist_id\", \"server_request_country_code\"]]\n",
    "df_january_2016_smaller.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4-column dataframe uses just under 200 MB of memory. That's a big saving! \n",
    "We can clean up the bigger version of the dataset to release the memory using Python's `del` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_january_2016  # Uncomment this line to delete the df_january_2016 Dataframe from memory. The df_january_2016 variable will be deleted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Could we somehow avoid loading the all columns to begin with? As it turns out, yes. One of the features that makes Parquet files efficient is that you can select which columns of the data you want to read without having to load all of the columns into memory first. (This is one factor makes them better than CSV files for large datasets.)\n",
    "\n",
    "That means that we can use `pandas.read_parquet` to only load certain columns by passing it a `columns` argument. Let's try it for a single Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_file = pd.read_parquet(\n",
    "    f\"s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/0000_part_00.parquet\",\n",
    "    columns=[\"datetime\", \"event_url\", \"panelist_id\", \"server_request_country_code\"]\n",
    ")\n",
    "df_one_file.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow you to easily specify the columns you want when loading more than one file, below is a new version of the `load_parquet_files_from_s3` function we defined above with an added `columns` parameter. Do use and adapt it in your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "\n",
    "def load_parquet_files_from_s3(glob_pattern, columns):\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "    files = s3.glob(glob_pattern)\n",
    "    print(f\"Found {len(files)} files.\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"Loading file {i + 1}/{len(files)} into a DataFrame ...\")\n",
    "        df = pd.read_parquet(f\"s3://{file}\", columns=columns)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "test_df = load_parquet_files_from_s3(\"s3://data-eng-datasets-404544469985/vod_clickstream/full/dt=2016-01-01/*.parquet\", columns=[\"event_url\"])\n",
    "test_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "You now have a few tools and tricks you can use to start exploring the clickstream dataset.\n",
    "\n",
    "Another key thing to take away from this is: **the amount of data a data analyst can efficiently analyse on their own machine is limited**. This is why in many organisations, data analysts are supported by data engineers, who set up the systems and preprocessing of data that makes analysis possible.\n",
    "\n",
    "For now though, don't worry if the sample you are able to work with is small. At this stage, you are just exploring what this dataset has to offer.\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "- [Pandas documentation: tips for working with large datasets in Pandas.](https://pandas.pydata.org/docs/user_guide/scale.html)\n",
    "- [tqdm - an easy to use progress bar library for Python](https://github.com/tqdm/tqdm). See [this section of their docs](https://github.com/tqdm/tqdm#ipython-jupyter-integration) for how to use it in Jupyter notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-to-data-analysis-K0pwVPgF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
